{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ee8c04",
   "metadata": {},
   "source": [
    "https://www.health.com/diseases-conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafb3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import unquote\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7db41641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义随机的useragent\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "# 设置代理ip\n",
    "ipdata = pd.read_excel('D:/Intern_Project/代理ip的地址2000.xlsx')\n",
    "ipdata.fillna(\"\",inplace = True)\n",
    "\n",
    "proxieslist = []\n",
    "\n",
    "for i in ipdata.index.values:\n",
    "    line = ipdata.loc[i,['HTTP']].to_dict()\n",
    "    proxieslist.append(line)\n",
    "    \n",
    "# randomproxy = random.choice(proxieslist)\n",
    "\n",
    "# 设置消息头\n",
    "headers_pc = {\n",
    "    'User-Agent': ua.random,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'ie': 'UTF-8',\n",
    "    'wd': 'Python'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b87a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeparams():\n",
    "    headers_pc = {\n",
    "    'User-Agent': ua.random,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "    }\n",
    "    randomproxy = random.choice(proxieslist)\n",
    "    #  访问服务器，获取cookie\n",
    "#     res = requests.get('https://www.health.com/diseases-conditions', headers=headers_pc, proxies= randomproxy,)\n",
    "#     # 将cookieJar数据转化为字典，s\n",
    "#     cookie_dict = requests.utils.dict_from_cookiejar(res.cookies)\n",
    "#     return headers_pc, randomproxy, cookie_dict\n",
    "    return headers_pc, randomproxy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b5ff2",
   "metadata": {},
   "source": [
    "# 1、先获取术语列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d67e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_url(url):\n",
    "    results = []\n",
    "    headers_pc, randomproxy = changeparams()\n",
    "    try:\n",
    "        requests.adapters.DEFAULT_RETRIES = 12 # 增加重连次数\n",
    "        s = requests.session()\n",
    "        s.keep_alive = False # 关闭多余连接\n",
    "\n",
    "        r = requests.get(url, headers=headers_pc, params=params, proxies=randomproxy, timeout=10)\n",
    "#             encoding = cchardet.detect(r.content)['encoding']\n",
    "#             html = r.content.decode(encoding)\n",
    "        r.encoding = 'utf-8'\n",
    "        html = r.text\n",
    "        \n",
    "        with open('D:/Intern_Project/6_health.com/术语列表页.html', 'w+', encoding='utf-8') as f:\n",
    "             f.write(html)\n",
    "\n",
    "        selector = Selector(text=html)\n",
    "        termUrl_list = selector.xpath(\n",
    "            \"//a[@class='link-list__link type-title--sm text-link']\")\n",
    "        #print(len(termUrl_list))\n",
    "        for termUrl in termUrl_list:\n",
    "            url = termUrl.xpath('./@href').extract()[0]\n",
    "            term = termUrl.xpath('./text()').extract()[0].strip()\n",
    "            # print(term)\n",
    "            tmp = {\n",
    "                'term': term,\n",
    "                'url': url\n",
    "            }\n",
    "            results.append(tmp)\n",
    "            # time.sleep()\n",
    "        # pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0453f290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取术语链接列表\n",
    "health_url = f'https://www.health.com/diseases-conditions'\n",
    "health_results = get_term_url(health_url)\n",
    "with open('D:/Intern_Project/6_health.com/术语链接列表.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(health_results, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41737f04",
   "metadata": {},
   "source": [
    "# 2、获取每个术语对应的文章链接列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ca385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用来测试哪些页面能简单获取到\n",
    "# headers_pc, randomproxy = changeparams()\n",
    "            \n",
    "# requests.adapters.DEFAULT_RETRIES = 12 # 增加重连次数\n",
    "# s = requests.session()\n",
    "# s.keep_alive = False # 关闭多余连接\n",
    "\n",
    "# # url = f'https://www.medicinenet.com/diseases_and_conditions/alpha_u.htm'\n",
    "# url = f'http://www.everydayhealth.com/'\n",
    "# # r = requests.get(url, headers=headers_pc, params=params, proxies=randomproxy, cookies=cookie_dict, timeout=10)\n",
    "# r = requests.get(url, headers=headers_pc, params=params, proxies=randomproxy, timeout=10)\n",
    "# #             encoding = cchardet.detect(r.content)['encoding']\n",
    "# #             html = r.content.decode(encoding)\n",
    "# r.encoding = 'utf-8'\n",
    "# html = r.text\n",
    "# with open('详情页.html', 'w+', encoding='utf-8') as f:\n",
    "#     f.write(html)\n",
    "\n",
    "\n",
    "# selector = Selector(text=html)\n",
    "# term_url_list = selector.xpath(\"//div[@class='AZ_results']/ul\").extract()\n",
    "# print(len(term_url_list))\n",
    "# for term_url in term_url_list:\n",
    "#     try:\n",
    "#         url = term_url.xpath('./li/a/@href').extract()[0]\n",
    "#         if url is None or len(url) == 0:\n",
    "#             print(\"失败\")\n",
    "#             continue\n",
    "#         term = term_url.xpath('./li/a/text()').extract()[0]\n",
    "#         print(url)\n",
    "#         print(term)\n",
    "#     except Exception as e:\n",
    "#         print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
