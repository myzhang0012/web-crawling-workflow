{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ee8c04",
   "metadata": {},
   "source": [
    "https://www.rxlist.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafb3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import unquote\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db41641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义随机的useragent\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "# 设置代理ip\n",
    "ipdata = pd.read_excel('D:/Intern_Project/代理ip的地址2000.xlsx')\n",
    "ipdata.fillna(\"\",inplace = True)\n",
    "\n",
    "proxieslist = []\n",
    "\n",
    "for i in ipdata.index.values:\n",
    "    line = ipdata.loc[i,['HTTP']].to_dict()\n",
    "    proxieslist.append(line)\n",
    "    \n",
    "# randomproxy = random.choice(proxieslist)\n",
    "\n",
    "# 设置消息头\n",
    "headers_pc = {\n",
    "    'User-Agent': ua.random,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "}\n",
    "\n",
    "# params = {\n",
    "#     'ie': 'UTF-8',\n",
    "#     'wd': 'Python'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09b87a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeparams():\n",
    "    headers_pc = {\n",
    "    'User-Agent': ua.random,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "    }\n",
    "    randomproxy = random.choice(proxieslist)\n",
    "    #  访问百度服务器，获取cookie\n",
    "    res = requests.get('https://www.rxlist.com/supplements/alpha_a.htm', headers=headers_pc, proxies= randomproxy,)\n",
    "    # 将cookieJar数据转化为字典，s\n",
    "    cookie_dict = requests.utils.dict_from_cookiejar(res.cookies)\n",
    "    return headers_pc, randomproxy, cookie_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8491aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_download(tag, headers_pc, randomproxy, cookie_dict, item):\n",
    "    # pbar.set_description(item['term'])\n",
    "    try:\n",
    "        url = item['url']\n",
    "        term = item['term'].strip().replace('/', ' ').replace('?', '').replace('\\\\', '').replace('*', ' ')\n",
    "        if os.path.exists(f\"D:/Intern_Project/8_RxList/{tag}_htmls/{term}.html\"):\n",
    "#         if os.path.exists(f\"D:/Intern_Project/5_everydayhealth/health_html/{newterm}.html\"):\n",
    "            pbar.update(1)    \n",
    "            return None \n",
    "#         time.sleep(random.randint(0, 5))\n",
    "        r = requests.get(url, headers=headers_pc, proxies=randomproxy, cookies=cookie_dict, timeout=10)\n",
    "        r.encoding = 'utf-8'\n",
    "        html = r.text\n",
    "        \n",
    "        with open(f\"D:/Intern_Project/8_RxList/{tag}_htmls/{term}.html\", 'w+', encoding='utf-8') as f:\n",
    "#         with open(f\"D:/Intern_Project/5_everydayhealth/health_html/{newterm}.html\", 'w+', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "    \n",
    "        pbar.update(1)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b632ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_monitor(task_list, mt_pool, results):\n",
    "    while len(task_list) > 0:\n",
    "        tmp_task_list = []\n",
    "        for future in task_list:\n",
    "            if future.done():\n",
    "                result = future.result()\n",
    "#                 results.extend(result)\n",
    "            else:\n",
    "                tmp_task_list.append(future)\n",
    "        \n",
    "        task_list = tmp_task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757ea42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 下载四种article_html\n",
    "tags = ['drugs', 'supplements', 'drugs_class', 'drugs_comparasion']\n",
    "\n",
    "for tag in tags:\n",
    "    print(tag)\n",
    "    with open(f'D:/Intern_Project/8_RxList/{tag}术语链接列表.json', 'r', encoding='utf-8') as f:\n",
    "            url_list = json.load(f)\n",
    "    \n",
    "    pbar = tqdm(total=len(url_list))\n",
    "    \n",
    "    all_craw_tasks = []\n",
    "    results = []\n",
    "    headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "    with ThreadPoolExecutor(max_workers=10) as mt_pool:\n",
    "        for item in url_list:\n",
    "            tmp_task = mt_pool.submit(html_download, tag, headers_pc, randomproxy, cookie_dict, item)\n",
    "            all_craw_tasks.append(tmp_task)\n",
    "#             time.sleep(0.5)\n",
    "        # 监测下载任务进展\n",
    "        task_monitor(all_craw_tasks, mt_pool, results)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bafa96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 756/756 [02:36<00:00,  6.84it/s]"
     ]
    }
   ],
   "source": [
    "tag = 'drugs_class'\n",
    "with open(f'D:/Intern_Project/8_RxList/{tag}术语链接列表.json', 'r', encoding='utf-8') as f:\n",
    "        url_list = json.load(f)\n",
    "\n",
    "pbar = tqdm(total=len(url_list))\n",
    "\n",
    "all_craw_tasks = []\n",
    "results = []\n",
    "headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "with ThreadPoolExecutor(max_workers=10) as mt_pool:\n",
    "    for item in url_list:\n",
    "        tmp_task = mt_pool.submit(html_download, tag, headers_pc, randomproxy, cookie_dict, item)\n",
    "        all_craw_tasks.append(tmp_task)\n",
    "#             time.sleep(0.5)\n",
    "    # 监测下载任务进展\n",
    "    task_monitor(all_craw_tasks, mt_pool, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b8e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
