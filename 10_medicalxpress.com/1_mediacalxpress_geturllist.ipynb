{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50ee8c04",
   "metadata": {},
   "source": [
    "https://medicalxpress.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296fc63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urllib3==1.25.11"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\n",
      "selenium 4.10.0 requires urllib3[socks]<3,>=1.26, but you have urllib3 1.25.11 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "Successfully installed urllib3-1.25.11\n"
     ]
    }
   ],
   "source": [
    "# !pip install urllib3==1.25.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bafb3218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import requests\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapy.selector import Selector\n",
    "from urllib.parse import unquote\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db41641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义随机的useragent\n",
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent()\n",
    "\n",
    "# 设置代理ip\n",
    "ipdata = pd.read_excel('D:/Intern_Project/代理ip的地址2000.xlsx')\n",
    "ipdata.fillna(\"\",inplace = True)\n",
    "\n",
    "proxieslist = []\n",
    "\n",
    "for i in ipdata.index.values:\n",
    "    line = ipdata.loc[i,['HTTP']].to_dict()\n",
    "    proxieslist.append(line)\n",
    "    \n",
    "# randomproxy = random.choice(proxieslist)\n",
    "\n",
    "# 设置消息头\n",
    "headers_pc = {\n",
    "    'User-Agent': ua.random,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'ie': 'UTF-8',\n",
    "    'wd': 'Python'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b87a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeparams():\n",
    "    headers_pc = {\n",
    "    'User-Agent': ua.random,\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7'\n",
    "    }\n",
    "    randomproxy = random.choice(proxieslist)\n",
    "    #  访问百度服务器，获取cookie\n",
    "    res = requests.get('https://www.medicinenet.com/', headers=headers_pc)\n",
    "    # 将cookieJar数据转化为字典，\n",
    "    cookie_dict = requests.utils.dict_from_cookiejar(res.cookies)\n",
    "    return headers_pc, randomproxy, cookie_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b5ff2",
   "metadata": {},
   "source": [
    "# 1、先获取Topics术语链接列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f18018",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "\n",
    "results = []\n",
    "\n",
    "topic_url = 'https://medicalxpress.com/'\n",
    "r = requests.get(topic_url, headers=headers_pc, proxies=randomproxy, cookies=cookie_dict, timeout=10)\n",
    "r.encoding = 'utf-8'\n",
    "html = r.text\n",
    "with open(f'D:/Intern_Project/12_medicalxpress/topics列表页.html', 'w+', encoding='utf-8') as f:\n",
    "    f.write(html)\n",
    "\n",
    "selector = Selector(text=html)\n",
    "termUrl_list = selector.xpath(\n",
    "    \"//ul[@class='header-articles-nav']/li[@class='nav-item']/a\")\n",
    "\n",
    "for termUrl in termUrl_list:\n",
    "    tmp_url = termUrl.xpath('./@href').extract()[0]\n",
    "    tmp_term = termUrl.xpath('.//text()').extract()[0]\n",
    "    # print(term)\n",
    "    tmp = {\n",
    "        'term': tmp_term,\n",
    "        'url': tmp_url\n",
    "    }\n",
    "    results.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5d119",
   "metadata": {},
   "source": [
    "# 2、获取Conditions术语链接列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditions_list(url, headers_pc, randomproxy, cookie_dict):\n",
    "    letter_result = []\n",
    "    try:\n",
    "        requests.adapters.DEFAULT_RETRIES = 20 # 增加重连次数\n",
    "        s = requests.session()\n",
    "        s.keep_alive = False # 关闭多余连接\n",
    "\n",
    "        r = requests.get(url, headers=headers_pc, proxies=randomproxy, cookies=cookie_dict, timeout=10)\n",
    "        r.encoding = 'utf-8'\n",
    "        html = r.text\n",
    "#         if os.path.exists(f\"D:/Intern_Project/8_RxList/{tag}_{letter}列表页.html\"):   \n",
    "#             return None\n",
    "#         with open(f'D:/Intern_Project/10_MedicineNet/列表页/{tag}_{letter}列表页.html', 'w+', encoding='utf-8') as f:\n",
    "#              f.write(html)\n",
    "\n",
    "        selector = Selector(text=html)\n",
    "        termUrl_list = selector.xpath(\n",
    "            \"//div[@class='container']/div[@class='text-column-4 mb-5']/p/a\")\n",
    "        #print(len(termUrl_list))\n",
    "        for termUrl in termUrl_list:\n",
    "            tmp_url = termUrl.xpath('./@href').extract()[0]\n",
    "            tmp_term = termUrl.xpath('.//text()').extract()[0]\n",
    "            # print(term)\n",
    "            tmp = {\n",
    "                'term': tmp_term,\n",
    "                'url': tmp_url\n",
    "            }\n",
    "            letter_result.append(tmp)\n",
    "            # time.sleep(1)\n",
    "        # pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(3)\n",
    "        headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "        get_conditions_list(url, headers_pc, randomproxy, cookie_dict)\n",
    "        mislist.append(url)\n",
    "    return letter_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed059e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "letters = [chr(i) for i in range(97, 123)] # 生成26个英文字母的列表\n",
    "mislist = []\n",
    "for letter in letters:\n",
    "    conditions_url = f'https://medicalxpress.com/conditions/?fl={letter}'\n",
    "    letter_result = get_conditions_list(conditions_url, headers_pc, randomproxy, cookie_dict)\n",
    "    time.sleep(3)\n",
    "    results.extend(letter_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dd23bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "print(mislist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8af70ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'D:/Intern_Project/12_medicalxpress/术语链接列表.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(results, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af9e536",
   "metadata": {},
   "source": [
    "# 3、开始获取每个术语全部文章的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eddb96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'D:/Intern_Project/12_medicalxpress/术语链接列表.json', 'r', encoding='utf-8') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6831a70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分一下json文件\n",
    "results1 = results[:200]\n",
    "results2 = results[200:400]\n",
    "results3 = results[400:]\n",
    "\n",
    "with open(f'D:/Intern_Project/12_medicalxpress/术语链接列表0_199.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(results1, indent=2, ensure_ascii=False))\n",
    "with open(f'D:/Intern_Project/12_medicalxpress/术语链接列表200_399.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(results2, indent=2, ensure_ascii=False))\n",
    "with open(f'D:/Intern_Project/12_medicalxpress/术语链接列表400_627.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(results3, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ef7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_url_list(url, headers_pc, randomproxy, cookie_dict, term, i):\n",
    "    article_result = []\n",
    "    try:\n",
    "        requests.adapters.DEFAULT_RETRIES = 20 # 增加重连次数\n",
    "        s = requests.session()\n",
    "        s.keep_alive = False # 关闭多余连接\n",
    "\n",
    "        r = requests.get(url, headers=headers_pc, cookies=cookie_dict, timeout=10)\n",
    "        r.encoding = 'utf-8'\n",
    "        html = r.text\n",
    "#         if os.path.exists(f\"D:/Intern_Project/8_RxList/{tag}_{letter}列表页.html\"):   \n",
    "#             return None\n",
    "        with open(f'D:/Intern_Project/12_medicalxpress/列表页/{term}_{i}列表页.html', 'w+', encoding='utf-8') as f:\n",
    "             f.write(html)\n",
    "        \n",
    "        # TODO：加个判断，如果文件大小小于2kb，也把丢失页面记录下来\n",
    "        size = round(os.path.getsize(f'D:/Intern_Project/12_medicalxpress/列表页/{term}_{i}列表页.html')/float(1024),2)\n",
    "        if size<2.0:\n",
    "            # time.sleep(random.randint(3,5))\n",
    "            headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "            get_article_url_list(url, headers_pc, randomproxy, cookie_dict)\n",
    "            mis_article_list.append(url)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO：为了提高效率，先下载下来，之后再解析？\n",
    "\n",
    "        selector = Selector(text=html)\n",
    "        titleUrl_list = selector.xpath(\n",
    "            \"//div[@class='sorted-article-content d-flex flex-column ie-flex-1']/h2/a\")\n",
    "        #print(len(termUrl_list))\n",
    "        for titleUrl in titleUrl_list:\n",
    "            tmp_url = titleUrl.xpath('./@href').extract()[0]\n",
    "            tmp_title = titleUrl.xpath('.//text()').extract()[0]\n",
    "            # print(term)\n",
    "            tmp = {\n",
    "                'title': tmp_title,\n",
    "                'url': tmp_url\n",
    "            }\n",
    "            article_result.append(tmp)\n",
    "            # time.sleep(1)\n",
    "        # pbar.update(1)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # time.sleep(random.randint(3,5))\n",
    "        headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "        get_article_url_list(url, headers_pc, randomproxy, cookie_dict)\n",
    "        mis_article_list.append(url)\n",
    "    return article_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b632ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_monitor(task_list, mt_pool, results):\n",
    "    while len(task_list) > 0:\n",
    "        tmp_task_list = []\n",
    "        for future in task_list:\n",
    "            if future.done():\n",
    "                result = future.result()\n",
    "                results.extend(result)\n",
    "            else:\n",
    "                tmp_task_list.append(future)\n",
    "        \n",
    "        task_list = tmp_task_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2252d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagenum(url, headers_pc, randomproxy, cookie_dict):\n",
    "    try:\n",
    "        requests.adapters.DEFAULT_RETRIES = 20 # 增加重连次数\n",
    "        s = requests.session()\n",
    "        s.keep_alive = False # 关闭多余连接\n",
    "\n",
    "        r = requests.get(url, headers=headers_pc, cookies=cookie_dict, timeout=10)\n",
    "        r.encoding = 'utf-8'\n",
    "        html = r.text\n",
    "#         if os.path.exists(f\"D:/Intern_Project/8_RxList/{tag}_{letter}列表页.html\"):   \n",
    "#             return None\n",
    "#         with open(f'D:/Intern_Project/10_MedicineNet/列表页/{tag}_{letter}列表页.html', 'w+', encoding='utf-8') as f:\n",
    "#              f.write(html)\n",
    "\n",
    "        selector = Selector(text=html)\n",
    "        pagenum = selector.xpath(\n",
    "            \"//div[@class='pagination-view mr-4']/span/text()\").extract()[0]\n",
    "        pagenum = int(pagenum)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # time.sleep(random.randint(3,5))\n",
    "        headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "        get_pagenum(url, headers_pc, randomproxy, cookie_dict)\n",
    "    return pagenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051449b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='medicalxpress.com', port=443): Max retries exceeded with url: /addiction-news/sort/date/all/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
      "HTTPSConnectionPool(host='medicalxpress.com', port=443): Max retries exceeded with url: /addiction-news/sort/date/all/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
      "HTTPSConnectionPool(host='medicalxpress.com', port=443): Max retries exceeded with url: /addiction-news/sort/date/all/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n",
      "HTTPSConnectionPool(host='medicalxpress.com', port=443): Max retries exceeded with url: /addiction-news/sort/date/all/ (Caused by SSLError(SSLEOFError(8, 'EOF occurred in violation of protocol (_ssl.c:1129)')))\n"
     ]
    }
   ],
   "source": [
    "mis_item_list = []\n",
    "mis_article_list = []\n",
    "\n",
    "all_craw_tasks = []\n",
    "article_results = []\n",
    "\n",
    "headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "\n",
    "# pbar = tqdm(total=len(results))\n",
    "with ThreadPoolExecutor(max_workers=10) as mt_pool:\n",
    "    for item in results:\n",
    "        pagenum = 0\n",
    "        headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "        start_url = (item['url'] + 'sort/date/all/')\n",
    "        start_url.replace('https','http')\n",
    "        term = item['term']\n",
    "        try:\n",
    "            pagenum = get_pagenum(start_url, headers_pc, randomproxy, cookie_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            mis_item_list.append(item)\n",
    "        for i in range(pagenum):\n",
    "            page_url = item['url'] + 'sort/date/all/' + f'page{i}.html'\n",
    "            page_url.replace('https','http')\n",
    "            tmp_task = mt_pool.submit(get_article_url_list, page_url, headers_pc, randomproxy, cookie_dict, term , i)\n",
    "            all_craw_tasks.append(tmp_task)\n",
    "            if i % 5 == 0:\n",
    "                time.sleep(random.randint(3,5))\n",
    "                headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "        # pbar.update(1)\n",
    "    \n",
    "    task_monitor(all_craw_tasks, mt_pool, article_results)\n",
    "    \n",
    "    with open(f'D:/Intern_Project/12_medicalxpress/文章链接列表.json', 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(article_results, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0744d7",
   "metadata": {},
   "source": [
    "# 用来测试哪些页面能简单获取到"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93862c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "headers_pc, randomproxy, cookie_dict = changeparams()\n",
    "            \n",
    "requests.adapters.DEFAULT_RETRIES = 12 # 增加重连次数\n",
    "s = requests.session()\n",
    "s.keep_alive = False # 关闭多余连接\n",
    "\n",
    "# url = f'https://www.medicinenet.com/diseases_and_conditions/alpha_u.htm'\n",
    "start_url = f'https://medicalxpress.com/alzheimer-dementia-news/' + 'sort/date/all/'\n",
    "# r = requests.get(url, headers=headers_pc, params=params, proxies=randomproxy, cookies=cookie_dict, timeout=10)\n",
    "try:\n",
    "    pagenum = get_pagenum(start_url, headers_pc, randomproxy, cookie_dict)\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "print(pagenum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "147994ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'D:/Intern_Project/12_medicalxpress/术语链接列表.json', 'r', encoding='utf-8') as f:\n",
    "    results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b9536e",
   "metadata": {},
   "outputs": [],
   "source": [
    "allarticle_result = []\n",
    "for i in range(pagenum):\n",
    "    page_url = 'https://medicalxpress.com/alzheimer-dementia-news/' + 'sort/date/all/' + f'page{i}.html'\n",
    "    term = 'alzheimer-dementia-news'\n",
    "    allarticle_result.extend(get_article_url_list(page_url, headers_pc, randomproxy, cookie_dict, term , i))\n",
    "    if i % 5 == 0:\n",
    "        time.sleep(2)\n",
    "        headers_pc, randomproxy, cookie_dict = changeparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7432faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cb93d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://www.baidu.com', headers = headers_pc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
